{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3f0fc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2554 Documents\n",
      "Loaded 10905 Judgments\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = \"walmart-amazon\"\n",
    "\n",
    "train_catalog_df = pd.read_csv(f\"data/{DATASET_NAME}/train_catalog.csv\")\n",
    "train_queries_df = pd.read_csv(f\"data/{DATASET_NAME}/train_queries.csv\")\n",
    "val_catalog_df = pd.read_csv(f\"data/{DATASET_NAME}/val_catalog.csv\")\n",
    "val_queries_df = pd.read_csv(f\"data/{DATASET_NAME}/val_queries.csv\")\n",
    "print(f\"Loaded {len(train_catalog_df.index)} Documents\")\n",
    "print(f\"Loaded {len(train_queries_df.index)} Judgments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f76e7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores': array([0.51749004, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy.random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class RandomRanker:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, catalog_df, queries_df):\n",
    "        pass\n",
    "    \n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = query[\"input_text\"]\n",
    "        return {\n",
    "            \"scores\": np.random.uniform(0,1,size=len(catalog_df))\n",
    "        }\n",
    "    \n",
    "def levenshtein_distance(word1, word2):\n",
    "    if len(word1) < len(word2):\n",
    "        return levenshtein_distance(word2, word1)\n",
    "\n",
    "    if len(word2) == 0:\n",
    "        return len(word1)\n",
    "\n",
    "    previous_row = range(len(word2) + 1)\n",
    "\n",
    "    for i, c1 in enumerate(word1):\n",
    "        current_row = [i + 1]\n",
    "\n",
    "        for j, c2 in enumerate(word2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "    \n",
    "\n",
    "def normalized_levenshtein_distance(word1, word2):\n",
    "    distance = levenshtein_distance(word1, word2)\n",
    "    max_length = max(len(word1), len(word2))\n",
    "    return distance / max_length\n",
    "\n",
    "    \n",
    "class LevensteinRanker:\n",
    "    def __init__(self):\n",
    "        print(\"Wanring! This is a slow ranker\")\n",
    "        pass\n",
    "    \n",
    "    def train(self, catalog_df, queries_df):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = query[\"input_text\"]\n",
    "        \n",
    "        return {\n",
    "            \"scores\": catalog_df[\"title\"].apply(lambda x: -normalized_levenshtein_distance(x, text)).values\n",
    "        }\n",
    "    \n",
    "class TfidfRanker:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
    "\n",
    "    def train(self, catalog_df, queries_df):\n",
    "        self.vectorizer.fit(catalog_df['title'].str.lower())\n",
    "\n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = query[\"input_text\"].lower()\n",
    "        tfidf_matrix = self.vectorizer.transform(catalog_df['title'].str.lower())\n",
    "        query_vector = self.vectorizer.transform([text])\n",
    "        scores = (tfidf_matrix * query_vector.T).toarray()\n",
    "        return {\n",
    "            \"scores\": scores.flatten()\n",
    "        }\n",
    "    \n",
    "tf_idf_ranker = TfidfRanker()\n",
    "tf_idf_ranker.train(train_catalog_df, train_queries_df)\n",
    "tf_idf_ranker.get_score({\"input_text\": \"Remote\"}, train_catalog_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9d23c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10905/10905 [00:15<00:00, 696.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 TF-IDF: 0.9900990099009901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10905/10905 [00:01<00:00, 10573.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Random: 0.0024752475247524753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(ranker, catalog_df, queries_df):\n",
    "    ranks = []\n",
    "    for i,row in tqdm(queries_df.iterrows(), total=len(queries_df.index)):\n",
    "        input_query = dict(row)\n",
    "        target_id = input_query[\"match_id\"]\n",
    "        judgment = input_query[\"judgment\"]\n",
    "        \n",
    "        if judgment == True:\n",
    "            del input_query[\"match_id\"]\n",
    "            del input_query[\"judgment\"]\n",
    "            \n",
    "            scores = ranker.get_score(input_query, catalog_df)[\"scores\"]\n",
    "#             print(catalog_df.iloc[np.argsort(scores)])\n",
    "            sorted_catalog = catalog_df.iloc[np.argsort(-scores)]\n",
    "            rank = np.where(sorted_catalog[\"subject_id\"].values == target_id)\n",
    "            rank = rank[0][0] # FIXME: This could file if target_id is not in the catalog_df, in that case, skip\n",
    "            ranks.append(rank)\n",
    "          \n",
    "    ranks = np.array(ranks)\n",
    "    return {\n",
    "        \"ranks\": ranks,\n",
    "        \"top_1\": sum(ranks < 1) / len(ranks),\n",
    "        \"top_10\": sum(ranks < 10) / len(ranks),\n",
    "        \"top_100\": sum(ranks < 100) / len(ranks),\n",
    "        \"top_1000\": sum(ranks < 1000) / len(ranks),\n",
    "    }\n",
    "            \n",
    "print(f'Top 10 TF-IDF: {evaluate(tf_idf_ranker, val_catalog_df, val_queries_df)[\"top_10\"]}')\n",
    "print(f'Top 10 Random: {evaluate(RandomRanker(), val_catalog_df, val_queries_df)[\"top_10\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9831f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10905/10905 [00:15<00:00, 692.79it/s]\n"
     ]
    }
   ],
   "source": [
    "res = evaluate(tf_idf_ranker, train_catalog_df, train_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c031df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
