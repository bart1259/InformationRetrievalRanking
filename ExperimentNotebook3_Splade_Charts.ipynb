{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DATASET_NAME\"] = \"covid-abstracts\" # choose from [\"news-aggregator\", \"covid-abstracts\", \"walmart-amazon\"]\n",
    "os.environ[\"AUGMENTATION\"] = \"none\" # choose from [\"synonym\", \"none\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T23:07:33.394439400Z",
     "start_time": "2024-05-01T23:07:33.378293500Z"
    }
   },
   "id": "7f24fd73c27990bc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1263247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T23:07:34.000210600Z",
     "start_time": "2024-05-01T23:07:33.860612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name is: covid-abstracts\n",
      "Loaded 6000 Documents\n",
      "Loaded 12000 Judgments\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if \"DATASET_NAME\" in os.environ:\n",
    "    DATASET_NAME = os.environ[\"DATASET_NAME\"]\n",
    "else:\n",
    "    DATASET_NAME= \"news-aggregator\"\n",
    "    \n",
    "print(f\"Dataset name is: {DATASET_NAME}\")\n",
    "\n",
    "train_catalog_df = pd.read_csv(f\"data/{DATASET_NAME}/train_catalog.csv\")\n",
    "train_queries_df = pd.read_csv(f\"data/{DATASET_NAME}/train_queries.csv\")\n",
    "val_catalog_df = pd.read_csv(f\"data/{DATASET_NAME}/val_catalog.csv\")\n",
    "val_queries_df = pd.read_csv(f\"data/{DATASET_NAME}/val_queries.csv\")\n",
    "print(f\"Loaded {len(train_catalog_df.index)} Documents\")\n",
    "print(f\"Loaded {len(train_queries_df.index)} Judgments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.set_default_dtype(torch.float32)\n",
    "print(use_cuda)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T23:07:36.207890800Z",
     "start_time": "2024-05-01T23:07:34.464398Z"
    }
   },
   "id": "9712e67d821f47b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Override\n",
    "#use_cuda = False\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T23:07:37.194453300Z",
     "start_time": "2024-05-01T23:07:37.174120200Z"
    }
   },
   "id": "9d78ad3f23e1c66f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df83cc46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T23:07:37.556426800Z",
     "start_time": "2024-05-01T23:07:37.506641100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation strategy is: none\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "if \"AUGMENTATION\" in os.environ:\n",
    "    AUGMENTATION = os.environ[\"AUGMENTATION\"]\n",
    "else:\n",
    "    AUGMENTATION = \"synonym\"\n",
    "    \n",
    "print(f\"Augmentation strategy is: {AUGMENTATION}\")\n",
    "    \n",
    "if AUGMENTATION == \"none\":\n",
    "    pass\n",
    "elif AUGMENTATION == \"synonym\":\n",
    "\n",
    "    import nlpaug.augmenter.word as naw\n",
    "\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', aug_p=0.25)\n",
    "    train_queries_df[\"input_text\"] = train_queries_df[\"input_text\"].apply(lambda x: aug.augment(x)[0])\n",
    "    val_queries_df[\"input_text\"] = val_queries_df[\"input_text\"].apply(lambda x: aug.augment(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data.dataloader import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T23:07:40.137680100Z",
     "start_time": "2024-05-01T23:07:38.119214400Z"
    }
   },
   "id": "5c9142fffc699b84"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338bbeec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T23:08:45.569920800Z",
     "start_time": "2024-05-01T23:07:40.118100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9586 Examples Found\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6b920ea64d44a6a94a03dff31007ac5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/600 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6a694247fb94352a4121da078b07146"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'scores': array([0., 0., 0., ..., 0., 0., 0.])}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomRanker:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, catalog_df, queries_df):\n",
    "        pass\n",
    "    \n",
    "    def prerun(self, catalog_df):\n",
    "        pass\n",
    "    \n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = query[\"input_text\"]\n",
    "        return {\n",
    "            \"scores\": np.random.uniform(0,1,size=len(catalog_df))\n",
    "        }\n",
    "    \n",
    "def levenshtein_distance(word1, word2):\n",
    "    if len(word1) < len(word2):\n",
    "        return levenshtein_distance(word2, word1)\n",
    "\n",
    "    if len(word2) == 0:\n",
    "        return len(word1)\n",
    "\n",
    "    previous_row = range(len(word2) + 1)\n",
    "\n",
    "    for i, c1 in enumerate(word1):\n",
    "        current_row = [i + 1]\n",
    "\n",
    "        for j, c2 in enumerate(word2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "    \n",
    "\n",
    "def normalized_levenshtein_distance(word1, word2):\n",
    "    distance = levenshtein_distance(word1, word2)\n",
    "    max_length = max(len(word1), len(word2))\n",
    "    return distance / max_length\n",
    "\n",
    "    \n",
    "class LevensteinRanker:\n",
    "    def __init__(self):\n",
    "        print(\"Wanring! This is a slow ranker\")\n",
    "        pass\n",
    "    \n",
    "    def train(self, catalog_df, queries_df):\n",
    "        pass\n",
    "    \n",
    "    def prerun(self, catalog_df):\n",
    "        pass\n",
    "    \n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = str(query[\"input_text\"])\n",
    "        \n",
    "        return {\n",
    "            \"scores\": catalog_df[\"text\"].apply(lambda x: -normalized_levenshtein_distance(x, text)).values\n",
    "        }\n",
    "    \n",
    "class BoWRanker:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
    "\n",
    "    def train(self, catalog_df, queries_df):\n",
    "        self.vectorizer.fit(catalog_df['text'].str.lower())\n",
    "\n",
    "    def prerun(self, catalog_df):\n",
    "        self.bow_matrix = self.vectorizer.transform(catalog_df['text'].str.lower())\n",
    "        \n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = str(query[\"input_text\"]).lower()\n",
    "        query_vector = self.vectorizer.transform([text])\n",
    "        scores = (self.bow_matrix * query_vector.T).toarray()\n",
    "        return {\n",
    "            \"scores\": scores.flatten()\n",
    "        }\n",
    "    \n",
    "class TfidfRanker:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
    "\n",
    "    def train(self, catalog_df, queries_df):\n",
    "        self.vectorizer.fit(catalog_df['text'].str.lower())\n",
    "\n",
    "    def prerun(self, catalog_df):\n",
    "        self.tfidf_matrix = self.vectorizer.transform(catalog_df['text'].str.lower())\n",
    "        \n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = str(query[\"input_text\"]).lower()\n",
    "        query_vector = self.vectorizer.transform([text])\n",
    "        scores = (self.tfidf_matrix * query_vector.T).toarray()\n",
    "        return {\n",
    "            \"scores\": scores.flatten()\n",
    "        }\n",
    "    \n",
    "class BM25Ranker:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def train(self, catalog_df, queries_df):\n",
    "        pass\n",
    "        \n",
    "    def prerun(self, catalog_df):\n",
    "        corpus = catalog_df['text'].str.lower().tolist()\n",
    "        tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    def get_score(self, query, catalog_df):\n",
    "        text = str(query[\"input_text\"]).lower()\n",
    "        query_vector = text.split(\" \")\n",
    "        scores = self.bm25.get_scores(query_vector)\n",
    "        return {\n",
    "            \"scores\": scores\n",
    "        }\n",
    "    \n",
    "class EmbeddingRanker:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def train(self, catalog_df, queries_df, epochs=0):\n",
    "        if epochs != 0:\n",
    "            \n",
    "            # Prepare the data for training\n",
    "            examples = []\n",
    "            for _, row in queries_df.iterrows():\n",
    "                text = str(row['input_text'])\n",
    "                positive_id = row['match_id']\n",
    "                try:\n",
    "                    positive_text = catalog_df.loc[catalog_df['catalog_id'] == positive_id, 'text'].values[0]\n",
    "                    negative_ids = catalog_df.loc[catalog_df['catalog_id'] != positive_id, 'catalog_id'].sample(n=1).values\n",
    "                    negative_text = catalog_df.loc[catalog_df['catalog_id'] == negative_ids[0], 'text'].values[0]\n",
    "                    examples.append(InputExample(texts=[text, positive_text, negative_text]))\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "            print(f\"{len(examples)} Examples Found\")\n",
    "\n",
    "            train_dataloader = DataLoader(examples, shuffle=True, batch_size=16)\n",
    "            train_loss = losses.TripletLoss(self.model)\n",
    "\n",
    "            self.model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=epochs, optimizer_params={'lr': 2e-6})\n",
    "\n",
    "    def prerun(self, catalog_df):\n",
    "        self.corpus = catalog_df['text'].str.lower().tolist()\n",
    "        self.corpus_embeddings = self.get_embeddings(self.corpus)\n",
    "\n",
    "    def get_score(self, query, catalog_df):\n",
    "        query_embedding = self.get_embeddings([query[\"input_text\"]])\n",
    "        scores = cosine_similarity(query_embedding, self.corpus_embeddings)\n",
    "        return {\n",
    "            \"scores\": scores.flatten()\n",
    "        }\n",
    "\n",
    "    def get_embeddings(self, texts):\n",
    "        return self.model.encode(texts)\n",
    "    \n",
    "embedding_ranker = EmbeddingRanker()\n",
    "embedding_ranker.train(train_catalog_df, train_queries_df, epochs=0)\n",
    "embedding_ranker.prerun(train_catalog_df)\n",
    "embedding_ranker.get_score({\"input_text\": \"Remote\"}, train_catalog_df)\n",
    "    \n",
    "trained_embedding_ranker_1 = EmbeddingRanker()\n",
    "trained_embedding_ranker_1.train(train_catalog_df, train_queries_df, epochs=1)\n",
    "trained_embedding_ranker_1.prerun(train_catalog_df)\n",
    "trained_embedding_ranker_1.get_score({\"input_text\": \"Remote\"}, train_catalog_df)\n",
    "\n",
    "bow_ranker = BoWRanker()\n",
    "bow_ranker.train(train_catalog_df, train_queries_df)\n",
    "bow_ranker.prerun(train_catalog_df)\n",
    "bow_ranker.get_score({\"input_text\": \"Remote\"}, train_catalog_df)\n",
    "\n",
    "tf_idf_ranker = TfidfRanker()\n",
    "tf_idf_ranker.train(train_catalog_df, train_queries_df)\n",
    "tf_idf_ranker.prerun(train_catalog_df)\n",
    "tf_idf_ranker.get_score({\"input_text\": \"Remote\"}, train_catalog_df)\n",
    "\n",
    "bm_25_ranker = BM25Ranker()\n",
    "bm_25_ranker.train(train_catalog_df, train_queries_df)\n",
    "bm_25_ranker.prerun(train_catalog_df)\n",
    "bm_25_ranker.get_score({\"input_text\": \"Remote\"}, train_catalog_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "class SpladeRanker:\n",
    "    def __init__(self, model_id='naver/splade-cocondenser-ensembledistil'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "        if use_cuda:\n",
    "            self.model.to(\"cuda:0\")\n",
    "        self.model.eval()\n",
    "\n",
    "        self.document_vectors = None\n",
    "\n",
    "    def train(self, catalog_df, queries_df):\n",
    "        pass\n",
    "\n",
    "    def prerun(self, catalog_df):\n",
    "        # Precompute and store SPLADE vectors for all documents in the catalog\n",
    "        texts = catalog_df['text'].tolist()\n",
    "        #self.document_vectors = [self.encode(text) for text in texts]\n",
    "        self.document_vectors = self.encode_batch(texts)\n",
    "    \n",
    "    def get_score(self, query, catalog_df):\n",
    "        # Encode the query using SPLADE\n",
    "        query_text = str(query[\"input_text\"])\n",
    "        query_vector = self.encode(query_text)\n",
    "\n",
    "        scores = [self.cosine_similarity(query_vector, doc_vector) for doc_vector in self.document_vectors]\n",
    "\n",
    "        return {\n",
    "            \"scores\": np.array(scores)\n",
    "        }\n",
    "\n",
    "    def encode(self, text):\n",
    "        #Text to SPLADE sparse vectors\n",
    "        tokens = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        if use_cuda:\n",
    "            tokens.to(\"cuda:0\")\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**tokens)\n",
    "\n",
    "        vec = torch.max(\n",
    "            torch.log(1 + torch.relu(output.logits)) * tokens.attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        )[0].squeeze()\n",
    "\n",
    "        return vec.cpu().numpy()\n",
    "    \n",
    "    def encode_batch(self, texts, batch_size=128):\n",
    "        #Batches of text to SPLADE sparse vectors, only works correctly for prerun\n",
    "        all_vecs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            tokens = self.tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "            if use_cuda:\n",
    "                tokens = tokens.to(\"cuda:0\")\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**tokens)\n",
    "            vecs = torch.max(\n",
    "                torch.log(1 + torch.relu(outputs.logits)) * tokens.attention_mask.unsqueeze(-1),\n",
    "                dim=1\n",
    "            )[0].squeeze()\n",
    "            all_vecs.extend(vecs.cpu().numpy())\n",
    "        return all_vecs\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_a = np.linalg.norm(vec1)\n",
    "        norm_b = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_a * norm_b)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T23:08:45.603562200Z",
     "start_time": "2024-05-01T23:08:45.565910800Z"
    }
   },
   "id": "ce0bfc437bdc677b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(ranker, catalog_df, queries_df):\n",
    "    ranks = []\n",
    "    ranker.prerun(catalog_df)\n",
    "    for i,row in tqdm(queries_df.iterrows(), total=len(queries_df.index)):\n",
    "        input_query = dict(row)\n",
    "        target_id = input_query[\"match_id\"]\n",
    "        judgment = input_query[\"judgment\"]\n",
    "        \n",
    "        if judgment == True:\n",
    "            del input_query[\"match_id\"]\n",
    "            del input_query[\"judgment\"]\n",
    "            \n",
    "            scores = ranker.get_score(input_query, catalog_df)[\"scores\"]\n",
    "            sorted_catalog = catalog_df.iloc[np.argsort(-scores)]\n",
    "            rank = np.where(sorted_catalog[\"catalog_id\"].values == target_id)\n",
    "            rank = rank[0][0] # FIXME: This could file if target_id is not in the catalog_df, in that case, skip\n",
    "            ranks.append(rank)\n",
    "          \n",
    "    ranks = np.array(ranks)\n",
    "    return {\n",
    "        \"ranks\": ranks,\n",
    "        \"top_1\": sum(ranks < 1) / len(ranks),\n",
    "        \"top_3\": sum(ranks < 3) / len(ranks),\n",
    "        \"top_5\": sum(ranks < 5) / len(ranks),\n",
    "        \"top_10\": sum(ranks < 10) / len(ranks),\n",
    "        \"top_25\": sum(ranks < 25) / len(ranks),\n",
    "        \"top_50\": sum(ranks < 50) / len(ranks),\n",
    "        \"top_100\": sum(ranks < 100) / len(ranks),\n",
    "        \"top_1000\": sum(ranks < 1000) / len(ranks),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T23:08:45.604576800Z",
     "start_time": "2024-05-01T23:08:45.583974500Z"
    }
   },
   "id": "4fc3e6033ea8bff0"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9a41c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T23:53:19.216629200Z",
     "start_time": "2024-05-01T23:53:15.902569100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 4302.81it/s]\n",
      "  8%|▊         | 328/4000 [00:00<00:04, 845.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m metrics \u001B[38;5;241m=\u001B[39m metrics_random\n\u001B[0;32m      5\u001B[0m report \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRandom | Top 1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_1\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 3: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_3\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 5: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_5\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 10: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_10\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 25: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_25\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 50: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_50\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 100: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_100\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m metrics_bow \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbow_ranker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_catalog_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_queries_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m metrics \u001B[38;5;241m=\u001B[39m metrics_bow\n\u001B[0;32m      9\u001B[0m report \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBag of Words | Top 1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_1\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 3: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_3\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 5: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_5\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 10: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_10\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 25: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_25\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 50: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_50\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Top 100: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_100\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[9], line 15\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(ranker, catalog_df, queries_df)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m input_query[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmatch_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m input_query[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjudgment\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m---> 15\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mranker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatalog_df\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     16\u001B[0m sorted_catalog \u001B[38;5;241m=\u001B[39m catalog_df\u001B[38;5;241m.\u001B[39miloc[np\u001B[38;5;241m.\u001B[39margsort(\u001B[38;5;241m-\u001B[39mscores)]\n\u001B[0;32m     17\u001B[0m rank \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(sorted_catalog[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcatalog_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;241m==\u001B[39m target_id)\n",
      "Cell \u001B[1;32mIn[7], line 77\u001B[0m, in \u001B[0;36mBoWRanker.get_score\u001B[1;34m(self, query, catalog_df)\u001B[0m\n\u001B[0;32m     75\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(query[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_text\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m     76\u001B[0m query_vector \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorizer\u001B[38;5;241m.\u001B[39mtransform([text])\n\u001B[1;32m---> 77\u001B[0m scores \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbow_matrix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mquery_vector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m)\u001B[38;5;241m.\u001B[39mtoarray()\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscores\u001B[39m\u001B[38;5;124m\"\u001B[39m: scores\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m     80\u001B[0m }\n",
      "File \u001B[1;32m~\\Desktop\\InformationRetrievalRanking\\venv-gpu\\lib\\site-packages\\scipy\\sparse\\_matrix.py:44\u001B[0m, in \u001B[0;36mspmatrix.__mul__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__mul__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_matmul_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\InformationRetrievalRanking\\venv-gpu\\lib\\site-packages\\scipy\\sparse\\_base.py:606\u001B[0m, in \u001B[0;36m_spbase._matmul_dispatch\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m    604\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m other\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    605\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCannot yet multiply a 1d sparse array\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 606\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_matmul_sparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    608\u001B[0m \u001B[38;5;66;03m# If it's a list or whatever, treat it like an array\u001B[39;00m\n\u001B[0;32m    609\u001B[0m other_a \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(other)\n",
      "File \u001B[1;32m~\\Desktop\\InformationRetrievalRanking\\venv-gpu\\lib\\site-packages\\scipy\\sparse\\_compressed.py:515\u001B[0m, in \u001B[0;36m_cs_matrix._matmul_sparse\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m    511\u001B[0m idx_dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_index_dtype((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindptr, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices,\n\u001B[0;32m    512\u001B[0m                              other\u001B[38;5;241m.\u001B[39mindptr, other\u001B[38;5;241m.\u001B[39mindices))\n\u001B[0;32m    514\u001B[0m fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(_sparsetools, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_matmat_maxnnz\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 515\u001B[0m nnz \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    516\u001B[0m \u001B[43m         \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindptr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midx_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    517\u001B[0m \u001B[43m         \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midx_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    518\u001B[0m \u001B[43m         \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindptr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midx_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[43m         \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midx_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    521\u001B[0m idx_dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_index_dtype((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindptr, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices,\n\u001B[0;32m    522\u001B[0m                              other\u001B[38;5;241m.\u001B[39mindptr, other\u001B[38;5;241m.\u001B[39mindices),\n\u001B[0;32m    523\u001B[0m                             maxval\u001B[38;5;241m=\u001B[39mnnz)\n\u001B[0;32m    525\u001B[0m indptr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(major_axis \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39midx_dtype)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "report = \"\"\n",
    "\n",
    "metrics_random = evaluate(RandomRanker(), val_catalog_df, val_queries_df)\n",
    "metrics = metrics_random\n",
    "report += (f'Random | Top 1: {metrics[\"top_1\"]} | Top 3: {metrics[\"top_3\"]} | Top 5: {metrics[\"top_5\"]} | Top 10: {metrics[\"top_10\"]} | Top 25: {metrics[\"top_25\"]} | Top 50: {metrics[\"top_50\"]} | Top 100: {metrics[\"top_100\"]}\\n')\n",
    "\n",
    "metrics_bow = evaluate(bow_ranker, val_catalog_df, val_queries_df)\n",
    "metrics = metrics_bow\n",
    "report += (f'Bag of Words | Top 1: {metrics[\"top_1\"]} | Top 3: {metrics[\"top_3\"]} | Top 5: {metrics[\"top_5\"]} | Top 10: {metrics[\"top_10\"]} | Top 25: {metrics[\"top_25\"]} | Top 50: {metrics[\"top_50\"]} | Top 100: {metrics[\"top_100\"]}\\n')\n",
    "\n",
    "metrics_tfidf = evaluate(tf_idf_ranker, val_catalog_df, val_queries_df)\n",
    "metrics = metrics_tfidf\n",
    "report += (f'TF-IDF | Top 1: {metrics[\"top_1\"]} | Top 3: {metrics[\"top_3\"]} | Top 5: {metrics[\"top_5\"]} | Top 10: {metrics[\"top_10\"]} | Top 25: {metrics[\"top_25\"]} | Top 50: {metrics[\"top_50\"]} | Top 100: {metrics[\"top_100\"]}\\n')\n",
    "\n",
    "metrics_bm25 = evaluate(bm_25_ranker, val_catalog_df, val_queries_df)\n",
    "metrics = metrics_bm25\n",
    "report += (f'BM25   | Top 1: {metrics[\"top_1\"]} | Top 3: {metrics[\"top_3\"]} | Top 5: {metrics[\"top_5\"]} | Top 10: {metrics[\"top_10\"]} | Top 25: {metrics[\"top_25\"]} | Top 50: {metrics[\"top_50\"]} | Top 100: {metrics[\"top_100\"]}\\n')\n",
    "\n",
    "metrics_sentence = evaluate(embedding_ranker, val_catalog_df, val_queries_df)\n",
    "metrics = metrics_sentence\n",
    "report += (f'Sentence Transformer | Top 1: {metrics[\"top_1\"]} | Top 3: {metrics[\"top_3\"]} | Top 5: {metrics[\"top_5\"]} | Top 10: {metrics[\"top_10\"]} | Top 25: {metrics[\"top_25\"]} | Top 50: {metrics[\"top_50\"]} | Top 100: {metrics[\"top_100\"]}\\n')\n",
    "\n",
    "metrics_fine_tuned_sentence = evaluate(trained_embedding_ranker_1, val_catalog_df, val_queries_df)\n",
    "metrics = metrics_fine_tuned_sentence\n",
    "report += (f'Fine Tuned Sentence Transformer | Top 1: {metrics[\"top_1\"]} | Top 3: {metrics[\"top_3\"]} | Top 5: {metrics[\"top_5\"]} | Top 10: {metrics[\"top_10\"]} | Top 25: {metrics[\"top_25\"]} | Top 50: {metrics[\"top_50\"]} | Top 100: {metrics[\"top_100\"]}\\n')\n",
    "\n",
    "splade_ranker = SpladeRanker()\n",
    "metrics_splade = evaluate(splade_ranker, val_catalog_df, val_queries_df)\n",
    "metrics = metrics_splade\n",
    "report += (f'SPLADE | Top 1: {metrics[\"top_1\"]} | Top 3: {metrics[\"top_3\"]} | Top 5: {metrics[\"top_5\"]} | Top 10: {metrics[\"top_10\"]} | Top 25: {metrics[\"top_25\"]} | Top 50: {metrics[\"top_50\"]} | Top 100: {metrics[\"top_100\"]}\\n')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random | Top 1: 0.001 | Top 3: 0.0015 | Top 5: 0.0025 | Top 10: 0.004 | Top 25: 0.0145 | Top 50: 0.0225 | Top 100: 0.0415\n",
      "Bag of Words | Top 1: 0.0495 | Top 3: 0.1015 | Top 5: 0.137 | Top 10: 0.1965 | Top 25: 0.3025 | Top 50: 0.411 | Top 100: 0.522\n",
      "TF-IDF | Top 1: 0.794 | Top 3: 0.905 | Top 5: 0.941 | Top 10: 0.9615 | Top 25: 0.978 | Top 50: 0.986 | Top 100: 0.989\n",
      "BM25   | Top 1: 0.8995 | Top 3: 0.949 | Top 5: 0.957 | Top 10: 0.97 | Top 25: 0.9825 | Top 50: 0.988 | Top 100: 0.9905\n",
      "Sentence Transformer | Top 1: 0.81 | Top 3: 0.9095 | Top 5: 0.933 | Top 10: 0.955 | Top 25: 0.977 | Top 50: 0.9905 | Top 100: 0.9965\n",
      "Fine Tuned Sentence Transformer | Top 1: 0.8255 | Top 3: 0.912 | Top 5: 0.9365 | Top 10: 0.9595 | Top 25: 0.979 | Top 50: 0.9915 | Top 100: 0.9965\n",
      "SPLADE | Top 1: 0.8715 | Top 3: 0.9415 | Top 5: 0.9545 | Top 10: 0.9735 | Top 25: 0.989 | Top 50: 0.992 | Top 100: 0.9965\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T23:52:29.732642800Z",
     "start_time": "2024-05-01T23:52:29.706755200Z"
    }
   },
   "id": "781c645ffe39d82b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db17f112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T23:52:35.694781300Z",
     "start_time": "2024-05-01T23:52:35.676675900Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Save run\n",
    "with open(f'outputs/report_{DATASET_NAME}_{AUGMENTATION}_{int(time.time())}.txt', 'w') as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf75fe",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T23:05:40.868995200Z"
    }
   },
   "outputs": [],
   "source": [
    "# def compare_rankers(ranker1, ranker2, catalog_df, queries_df, cutoff=10):\n",
    "#     ranks = []\n",
    "#     ranker1.prerun(catalog_df)\n",
    "#     ranker2.prerun(catalog_df)\n",
    "#     for i,row in tqdm(queries_df.iterrows(), total=len(queries_df.index)):\n",
    "#         input_query = dict(row)\n",
    "#         target_id = input_query[\"match_id\"]\n",
    "#         judgment = input_query[\"judgment\"]\n",
    "        \n",
    "#         if judgment == True:\n",
    "#             del input_query[\"match_id\"]\n",
    "#             del input_query[\"judgment\"]\n",
    "            \n",
    "#             scores1 = ranker1.get_score(input_query, catalog_df)[\"scores\"]\n",
    "#             scores2 = ranker2.get_score(input_query, catalog_df)[\"scores\"]\n",
    "#             sorted_catalog1 = catalog_df.iloc[np.argsort(-scores1)]\n",
    "#             sorted_catalog2 = catalog_df.iloc[np.argsort(-scores2)]\n",
    "#             rank1 = np.where(sorted_catalog1[\"catalog_id\"].values == target_id)\n",
    "#             rank1 = rank1[0][0]\n",
    "#             rank2 = np.where(sorted_catalog2[\"catalog_id\"].values == target_id)\n",
    "#             rank2 = rank2[0][0]\n",
    "            \n",
    "#             if rank1 < cutoff and rank2 > cutoff:\n",
    "#                 print(\"Ranker 1 was better at matching \\\"\" + input_query[\"input_text\"] + \"\\\" to \\\"\" + catalog_df[catalog_df[\"catalog_id\"] == target_id][\"text\"].values[0] + \"\\\"\")\n",
    "#             if rank2 < cutoff and rank1 > cutoff:\n",
    "#                 print(\"Ranker 2 was better at matching \\\"\" + input_query[\"input_text\"] + \"\\\" to \\\"\" + catalog_df[catalog_df[\"catalog_id\"] == target_id][\"text\"].values[0] + \"\\\"\")\n",
    "          \n",
    "# compare_rankers(tf_idf_ranker, trained_embedding_ranker_1, val_catalog_df, val_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26295015",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555731d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = evaluate(bm_25_ranker, val_catalog_df, val_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3bd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cbd15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
